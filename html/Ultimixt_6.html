<div class="container">

<table style="width: 100%;"><tr>
<td>SM.MixPois</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
summary of the output produced by K.MixPois
</h2>

<h3>Description</h3>

<p>This generic function summarizes the MCMC samples produced by K.MixPois when several estimation methods have been invoked depending on the unimodality or multimodality of the argument.  
</p>


<h3>Usage</h3>

<pre><code class="language-R">SM.MixPois(estimate, xobs)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>estimate</code></td>
<td>

<p>output of K.MixPois
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xobs</code></td>
<td>

<p>vector of observations
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The output of this function contains posterior point estimates for all parameters of the reparameterized Poisson mixture model.
It summarizes unimodal MCMC samples by computing
measures of centrality, including mean and median, while multimodal outputs require a preprocessing, due to the
label switching phenomenon (Jasra et al., 2005). The summary measures are then computed after performing a multi-dimensional k-means clustering (Hartigan and Wong, 1979) following the suggestion of Fruhwirth-Schnatter (2006).
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>lambda </code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the mixture model mean</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma.i </code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the component mean hyperparameters; <code class="reqn">i=1, \ldots, k</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight.i </code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the component weights of the mixture distribution; <code class="reqn">i=1, \ldots, k</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.i </code></td>
<td>
<p>vector of mean and median of simulated draws from the conditional posterior of the component means of the mixture distribution; <code class="reqn">i=1, \ldots, k</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Acc rat </code></td>
<td>
<p>vector of final acceptance rate of the proposal distributions of the algorithm with no calibration
stage for the proposal scales</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Opt scale </code></td>
<td>
<p>vector of optimal proposal scales obtained the by calibration stage</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>For multimodal outputs such as the mixture model weights, component means, and component mean hyperparameters, for each MCMC draw,
first the labels of the weights <code class="reqn">p_i, i=1, \ldots, k</code> and corresponding component means are permuted in
such a way that <code class="reqn">p_1\le \ldots \le p_k</code>. Then the posterior component means are partitioned into <code class="reqn">k</code>
clusters by applying a standard k-means algorithm with <code class="reqn">k</code> clusters, following Fruhwirth-Schnatter (2006) method. The obtained classification sequence was then used to reorder and identify the other component-specific parameters, namely component mean hyperparameters and weights.
For each group,  cluster centers are considered as parameter
estimates. 
</p>


<h3>Author(s)</h3>

<p>Kaniav Kamary
</p>


<h3>References</h3>

<p>Jasra, A., Holmes, C. and Stephens, D. (2005). Markov Chain Monte Carlo methods and the label switching problem in
Bayesian mixture modeling. Statistical Science, 20, 50–67.
</p>
<p>Hartigan, J. A. and Wong, M. A. (1979). A K-means clustering algorithm. Applied Statistics 28, 100–108.
</p>
<p>Fruhwirth-Schnatter, S. (2006). Finite mixture and Markov switching models. Springer-Verlag.
</p>


<h3>See Also</h3>

<p><code>K.MixPois</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">N=500
U =runif(N)                                            
xobs = rep(NA,N)
for(i in 1:N){
    if(U[i]&lt;.6){
        xobs[i] = rpois(1,lambda=1)
    }else{
        xobs[i] = rpois(1,lambda=5)
    }
}
#estimate=K.MixPois(xobs, k=2, alpha0=.5, alpha=.5, Nsim=10000)
#SM.MixPois(estimate, xobs)
#plot(estimate[[8]][,1],estimate[[2]][,1],pch=19,col="skyblue",cex=0.5,xlab="lambda",ylab="p")
#points(estimate[[8]][,2], estimate[[2]][,2], pch=19, col="gold", cex=0.5)
#points(c(1,5), c(0.6,0.4), pch=19, cex=1)
</code></pre>


</div>