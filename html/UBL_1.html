<div class="container">

<table style="width: 100%;"><tr>
<td>UBL-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
UBL: Utility-Based Learning
</h2>

<h3>Description</h3>

<p>The package provides a diversity of pre-processing functions to deal with both classification (binary and multi-class) and regression problems that encompass non-uniform costs and/or benefits. These functions can be used to obtain a better predictive performance on this type of tasks. 
The package also includes two synthetic data sets for regression and classification.
</p>


<h3>Details</h3>


<table>
<tr>
<td style="text-align: left;">
Name: </td>
<td style="text-align: left;"> UBL</td>
</tr>
<tr>
<td style="text-align: left;">
Type: </td>
<td style="text-align: left;"> Package</td>
</tr>
<tr>
<td style="text-align: left;">
Version: </td>
<td style="text-align: left;"> 0.0.9</td>
</tr>
<tr>
<td style="text-align: left;">

Date: </td>
<td style="text-align: left;"> 2023-10-07</td>
</tr>
<tr>
<td style="text-align: left;">
License: </td>
<td style="text-align: left;"> GPL 2 GPL 3</td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<p>The package in focused on utility-based learning, i.e., classification and regression problems with non-uniform benefits and/or costs. The main goal of the implemented functions is to improve the predictive performance of the models obtained. The package provides pre-processing approaches that change the original data set biasing it towards the user preferences.
</p>
<p>All the methods avaliable are suitable for classification (binary and multiclass) and regression tasks. Moreover, several distance functions are also implemented which allows the use of the methods in data sets with categorical and/or numeric features. 
</p>
<p>We also provide two synthetic data sets for classification and regression.
</p>


<h3>Author(s)</h3>

<p> Paula Branco <a href="mailto:paobranco@gmail.com">paobranco@gmail.com</a>, Rita Ribeiro
<a href="mailto:rpribeiro@dcc.fc.up.pt">rpribeiro@dcc.fc.up.pt</a> and Luis Torgo <a href="mailto:ltorgo@dcc.fc.up.pt">ltorgo@dcc.fc.up.pt</a> 
</p>
<p>Maintainer:
Paula Branco</p>


<h3>References</h3>

<p>Branco, P., Ribeiro, R.P. and Torgo, L. (2016) <em>UBL: an R package for Utility-based Learning.</em> arXiv preprint arXiv:1604.08079.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
library(UBL)
# an example with the synthetic classification data set provided with the package
data(ImbC)

plot(ImbC$X1, ImbC$X2, col = ImbC$Class, xlab = "X1", ylab = "X2")

summary(ImbC)
# randomly generate a 30-70% test and train partition 
i.train &lt;- sample(1:nrow(ImbC), as.integer(0.7*nrow(ImbC)))
trainD &lt;- ImbC[i.train,]
testD &lt;- ImbC[-i.train,]

model &lt;- rpart(Class~., trainD)
preds &lt;- predict(model, testD, type = "class")
table(preds, testD$Class)

# apply random over-sampling approach to balance the data set:

newTrain &lt;- RandOverClassif(Class~., trainD)

newModel &lt;- rpart(Class~., newTrain)
newPreds &lt;- predict(newModel, testD, type = "class")
table(newPreds, testD$Class)


# an example with the synthetic regression data set provided with the package
data(ImbR)

library(ggplot2)
ggplot(ImbR, aes(x = X1, y = X2)) + geom_point(data = ImbR, aes(colour=Tgt)) +
      scale_color_gradient(low = "red", high="blue")

boxplot(ImbR$Tgt)
#relevance function automatically obtained
phiF.args &lt;- phi.control(ImbR$Tgt, method = "extremes", extr.type = "high")
y.phi &lt;- phi(sort(ImbR$Tgt),control.parms = phiF.args)

plot(sort(ImbR$Tgt), y.phi, type = "l", xlab = "Tgt variable", ylab = "relevance value")

# set the train and test data
i.train &lt;- sample(1:nrow(ImbR), as.integer(0.7*nrow(ImbR)))
trainD &lt;- ImbR[i.train,]
testD &lt;- ImbR[-i.train,]

# train a model on the original train data
  if (requireNamespace("DMwR2", quietly = TRUE)) {
model &lt;- DMwR2::rpartXse(Tgt~., trainD, se = 0)

preds &lt;- DMwR2::predict(model, testD)

plot(testD$Tgt, preds, xlim = c(0,55), ylim = c(0,55))
abline(a = 0, b = 1)

# obtain a new train using random under-sampling strategy
newTrain &lt;- RandUnderRegress(Tgt~., trainD)
newModel &lt;- DMwR2::rpartXse(Tgt~., newTrain, se = 0)
newPreds &lt;- DMwR2::predict(newModel, testD)

# plot the predictions for the model obtained with 
# the original and the modified train data
plot(testD$Tgt, preds, xlim = c(0,55), ylim = c(0,55)) #black for original train
abline(a = 0, b = 1, lty=2, col="grey")
points(testD$Tgt, newPreds, col="blue", pch=2) #blue for changed train
abline(h=30, lty=2, col="grey")
abline(v=30, lty=2, col="grey")
}

## End(Not run)
</code></pre>


</div>